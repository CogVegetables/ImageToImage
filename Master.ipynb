{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import mynn\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import mygrad as mg\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from mynn.layers.dense import dense\n",
    "from mynn.optimizers.adam import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from mygrad.nnet.losses.margin_ranking_loss import margin_ranking_loss\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "\n",
    "import pickle\n",
    "\n",
    "import re, string\n",
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "\n",
    "path = r\"./glove.6B.50d.txt.w2v\"\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOMappings:\n",
    "    def __init__(self, glove):\n",
    "        with open(r'./captions_train2014.json') as json_file:\n",
    "            self.data = json.load(json_file)\n",
    "            \n",
    "        self.glove = glove\n",
    "            \n",
    "        self.caption_ids_to_captions = {cap['id']: cap['caption'] for cap in self.data['annotations']}\n",
    "        self.image_ids_to_urls = {img['id']: img['coco_url'] for img in self.data['images']}\n",
    "        \n",
    "        self.img_ids = tuple(cap['image_id'] for cap in self.data['annotations'])\n",
    "        self.caption_ids = tuple(cap['id'] for cap in self.data['annotations'])\n",
    "        self.captions = tuple(cap['caption'] for cap in self.data['annotations'])\n",
    "        \n",
    "        self.caption_ids_to_embs = self.caption_id_to_emb()\n",
    "        self.image_id_to_caption_id = self.generate_image_to_caption_id()\n",
    "    \n",
    "    def generate_image_to_caption_id(self):\n",
    "        image_id_to_caption_ids_dict = {}\n",
    "        \n",
    "        for cap_dict in self.data['annotations']:\n",
    "            if cap_dict['image_id'] in image_id_to_caption_ids_dict:\n",
    "                image_id_to_caption_ids_dict[cap_dict['image_id']].append(cap_dict['id'])\n",
    "            else:\n",
    "                image_id_to_caption_ids_dict[cap_dict['image_id']] = [cap_dict['id']]\n",
    "                \n",
    "        return image_id_to_caption_ids_dict\n",
    "        \n",
    "    def tokenize(self, corpus):\n",
    "        return punc_regex.sub('', corpus).lower().split()\n",
    "\n",
    "    def to_df(self, captions):\n",
    "        \n",
    "        counter = Counter()\n",
    "        for caption in captions:\n",
    "            counter.update(set(self.tokenize(caption)))\n",
    "        return dict(counter)\n",
    "    \n",
    "    \n",
    "    def to_idf(self, captions):\n",
    "        \"\"\" \n",
    "        Given the vocabulary, and the word-counts for each document, computes\n",
    "        the inverse document frequency (IDF) for each term in the vocabulary.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        vocab : Sequence[str]\n",
    "            Ordered list of words that we care about.\n",
    "\n",
    "        counters : Iterable[collections.Counter]\n",
    "            The word -> count mapping for each document.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            An array whose entries correspond to those in `vocab`, storing\n",
    "            the IDF for each term `t`: \n",
    "                               log10(N / nt)\n",
    "            Where `N` is the number of documents, and `nt` is the number of \n",
    "            documents in which the term `t` occurs.\n",
    "        \"\"\"\n",
    "        vishnu = self.to_df(captions)\n",
    "        return {word : np.log10(len(captions)/cnt + 1) for word, cnt in vishnu.items()}\n",
    "    \n",
    "    def normalize(self, array):\n",
    "        #sqrroot(sum(vectorsquared))\n",
    "        return (sum(array ** 2)) ** 0.5\n",
    "        \n",
    "        \n",
    "    def caption_to_emb(self, caption, idfs):\n",
    "        vishnu = sum(self.glove[word] * idfs[word] for word in self.tokenize(caption) if word in self.glove)\n",
    "        return vishnu/self.normalize(vishnu)\n",
    "    \n",
    "    def caption_id_to_emb(self):\n",
    "        idfs = self.to_idf(self.captions)\n",
    "        return {caption_id : self.caption_to_emb(self.caption_ids_to_captions[caption_id], idfs) for caption_id in self.caption_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triples(data, model, num_captions, trips_per_cap):\n",
    "    triples = []\n",
    "    for i in range(num_captions):\n",
    "        img_id = np.random.choice(data)\n",
    "        cap_id = np.random.choice(model.image_id_to_caption_id[img_id])\n",
    "        cap_emb = model.caption_ids_to_embs[cap_id]\n",
    "        for n in range(trips_per_cap):\n",
    "            bad_img_id = generate_bad_img(img_id, cap_emb, model, data)\n",
    "            triples.append((img_id, cap_id, bad_img_id))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bad_img(img_id, cap_emb, model, data):\n",
    "    captions = []\n",
    "    images = []\n",
    "    for i in range(25):\n",
    "        id_choice = img_id\n",
    "        while id_choice == img_id: id_choice = np.random.choice(data)\n",
    "        images.append(id_choice)\n",
    "        captions.append(np.random.choice(model.image_id_to_caption_id[id_choice]))\n",
    "    dots = np.zeros(25)\n",
    "    for index, cap in enumerate(captions):\n",
    "        emb = model.caption_ids_to_embs[cap]\n",
    "        dots[index] = np.matmul(emb, cap_emb)\n",
    "    max_index = np.argmax(dots)\n",
    "    #bad_caption_id = captions[max_index]\n",
    "    bad_img_id = images[max_index]\n",
    "    return bad_img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEncoder:\n",
    "    def __init__(self, d_input, d_output):\n",
    "        \"\"\" This initializes all of the layers in our model, and sets them\n",
    "        as attributes of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        d_input : int\n",
    "            The size of the inputs.\n",
    "            \n",
    "        d_output : int\n",
    "            The size of the outputs (i.e., the reduced dimensionality).\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encoder = dense(d_input, d_output, weight_initializer=glorot_normal)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        '''Passes data as input to our model, performing a \"forward-pass\".\n",
    "        \n",
    "        This allows us to conveniently initialize a model `m` and then send data through it\n",
    "        to be classified by calling `m(x)`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Union[numpy.ndarray, mygrad.Tensor], shape=(M, D_full)\n",
    "            A batch of data consisting of M pieces of data,\n",
    "            each with a dimentionality of D_full.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mygrad.Tensor, shape=(M, D_full)\n",
    "            The model's prediction for each of the M pieces of data.\n",
    "        '''\n",
    "        \n",
    "        return self.encoder(x) \n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        \"\"\" A convenience function for getting all the parameters of our model.\n",
    "        \n",
    "        This can be accessed as an attribute, via `model.parameters` \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, ...]\n",
    "            A tuple containing all of the learnable parameters for our model \"\"\"\n",
    "        \n",
    "        return self.encoder.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco = COCOMappings(glove)\n",
    "total_imgs = len(coco.img_ids)\n",
    "train_range = 4*total_imgs//5\n",
    "ids = np.array(coco.img_ids)\n",
    "np.random.shuffle(ids)\n",
    "train = ids[:train_range]\n",
    "test = ids[train_range:]\n",
    "model = LinearEncoder(d_input=512, d_output=50)\n",
    "optimizer = Adam(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noggin import create_plot\n",
    "plotter, fig, ax = create_plot([\"loss\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 500\n",
    "\n",
    "for k in range(100000):\n",
    "    \n",
    "    output = model(x)\n",
    "        \n",
    "    loss = margin_ranking_loss(s_good, s_bad, y=1)\n",
    "    \n",
    "    acc = float(np.argmax(output.data.squeeze()) == target.item())\n",
    "\n",
    "    plotter.set_train_batch({\"loss\":loss.item(), \"accuracy\":acc}, batch_size=1, plot=False)\n",
    "    \n",
    "    if k % plot_every == 0 and k > 0:\n",
    "        plotter.set_train_epoch()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss.null_gradients()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
